{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from raug.metrics import Metrics, accuracy\n",
    "from raug.utils.loader import get_labels_frequency\n",
    "import pandas as pd\n",
    "\n",
    "# Especificar o caminho para a pasta \"best_results\"\n",
    "folder_path = \"best_results\"\n",
    "_csv_path_test = \"/home/a52550/Desktop/datasets/ISIC2017/test/ISIC-2017_Test_v2_Part3_GroundTruth.csv\"\n",
    "true_labels = pd.read_csv(_csv_path_test).sort_values(by=['image_id'])['category']\n",
    "\n",
    "# Inicializar as variáveis para armazenar as previsões dos modelos\n",
    "ensemble_preds_best = None\n",
    "ensemble_preds_last = None\n",
    "num_models = 0\n",
    "ser_lab_freq = get_labels_frequency(_csv_path_test, \"category\", \"image_id\")\n",
    "_labels_name = ser_lab_freq.index.values #np.reshape(ser_lab_freq.index.values, (ser_lab_freq.index.values.shape[0], 1))\n",
    "\n",
    "df_best = pd.DataFrame()\n",
    "df_last = pd.DataFrame()\n",
    "\n",
    "# Percorrer as pastas de cada experimento\n",
    "for experiment_folder in os.listdir(folder_path):\n",
    "\n",
    "    if \".csv\" in experiment_folder:\n",
    "        continue\n",
    "\n",
    "    print(\"Experimento: \", experiment_folder)\n",
    "\n",
    "    _metric_options_best = {\n",
    "        'save_all_path': os.path.join(folder_path, experiment_folder, 'ensemble_average/best'),\n",
    "        'pred_name_scores': 'predictions.csv',\n",
    "    }\n",
    "    _metric_options_last = {\n",
    "        'save_all_path': os.path.join(folder_path, experiment_folder, 'ensemble_average/last'),\n",
    "        'pred_name_scores': 'predictions.csv',\n",
    "    }\n",
    "\n",
    "\n",
    "    experiment_path = os.path.join(folder_path, experiment_folder)\n",
    "    metrics_best = Metrics ([\"accuracy\", \"topk_accuracy\", \"balanced_accuracy\", \"conf_matrix\" \"plot_conf_matrix\", \"precision_recall_report\", \"auc_and_roc_curve\", \"auc\"] , _labels_name, _metric_options_best)\n",
    "    metrics_last = Metrics ([\"accuracy\", \"topk_accuracy\", \"balanced_accuracy\", \"conf_matrix\" \"plot_conf_matrix\", \"precision_recall_report\", \"auc_and_roc_curve\", \"auc\"] , _labels_name, _metric_options_last)\n",
    "\n",
    "    ensemble_preds_best = None\n",
    "    ensemble_preds_last = None\n",
    "\n",
    "    model_folders = os.listdir(experiment_path)\n",
    "    \n",
    "    # Cria path para o experimento atual\n",
    "    if 'ensemble_average' in model_folders:\n",
    "        model_folders.remove('ensemble_average')\n",
    "    else:\n",
    "        os.mkdir(os.path.join(folder_path, experiment_folder, 'ensemble_average'))\n",
    "        os.mkdir(os.path.join(_metric_options_best['save_all_path']))\n",
    "        os.mkdir(os.path.join(_metric_options_last['save_all_path']))\n",
    "\n",
    "    if 'ensemble_3_best_average' in model_folders:\n",
    "        model_folders.remove('ensemble_3_best_average')\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Percorrer as pastas de cada modelo\n",
    "        for model_folder in model_folders:\n",
    "\n",
    "            model_path = os.path.join(experiment_path, model_folder)\n",
    "\n",
    "            # Carregar as previsões do modelo atual\n",
    "            model_preds_path_best = os.path.join(model_path, \"test_pred_best/predictions.csv\")\n",
    "            model_preds_path_last = os.path.join(model_path, \"test_pred_last/predictions.csv\")\n",
    "\n",
    "            model_preds_best = pd.read_csv(model_preds_path_best).sort_values(by=['image'])\n",
    "            model_preds_last = pd.read_csv(model_preds_path_last).sort_values(by=['image'])\n",
    "\n",
    "            images_id = model_preds_best[['image']]\n",
    "            model_preds_best = model_preds_best[['0','1','2']]\n",
    "            model_preds_last = model_preds_last[['0','1','2']]\n",
    "\n",
    "            # Adicionar as previsões do modelo atual ao ensemble\n",
    "            if ensemble_preds_best is None:\n",
    "                ensemble_preds_best = model_preds_best\n",
    "            else:\n",
    "                ensemble_preds_best += model_preds_best\n",
    "\n",
    "            if ensemble_preds_last is None:\n",
    "                ensemble_preds_last = model_preds_last\n",
    "            else:\n",
    "                ensemble_preds_last += model_preds_last\n",
    "\n",
    "            num_models += 1\n",
    "\n",
    "        # Calcular a média das previsões dos modelos\n",
    "        ensemble_preds_best /= num_models\n",
    "        ensemble_preds_last /= num_models\n",
    "\n",
    "        metrics_best.update_scores(true_labels, ensemble_preds_best.to_numpy(), images_id.to_numpy())\n",
    "        metrics_last.update_scores(true_labels, ensemble_preds_last.to_numpy(), images_id.to_numpy())\n",
    "\n",
    "        metrics_best.compute_metrics()\n",
    "        metrics_last.compute_metrics()\n",
    "\n",
    "        metrics_best.save_scores()\n",
    "        metrics_last.save_scores()\n",
    "\n",
    "        folder_name = {'folder_name': experiment_folder}\n",
    "        dict_best = {**folder_name, **metrics_best.metrics_values}\n",
    "        dict_last = {**folder_name, **metrics_last.metrics_values}\n",
    "\n",
    "        del dict_best['auc_and_roc_curve']\n",
    "        del dict_best['precision_recall_report']\n",
    "\n",
    "        del dict_last['auc_and_roc_curve']\n",
    "        del dict_last['precision_recall_report']\n",
    "\n",
    "        df_best = df_best.append(pd.DataFrame(dict_best, columns=dict_best.keys(), index=[0]), ignore_index=True)\n",
    "        df_last = df_last.append(pd.DataFrame(dict_last, columns=dict_last.keys(), index=[0]), ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erro no experimento \", experiment_folder)\n",
    "        print(e)\n",
    "\n",
    "\n",
    "df_best.to_csv(os.path.join(folder_path, 'ensemble_average_best.csv'), index=False)\n",
    "df_last.to_csv(os.path.join(folder_path, 'ensemble_average_last.csv'), index=False)\n",
    "\n",
    "# Avaliar a precisão do modelo de ensemble\n",
    "# true_classes_path = os.path.join(folder_path, \"true_classes.csv\")\n",
    "# true_classes = np.loadtxt(_csv_path_test, delimiter=\",\", skiprows=1)\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# accuracy = accuracy_score(true_classes, ensemble_classes)\n",
    "# precision = precision_score(true_classes, ensemble_classes, average=\"macro\")\n",
    "# recall = recall_score(true_classes, ensemble_classes, average=\"macro\")\n",
    "# f1 = f1_score(true_classes, ensemble_classes, average=\"macro\")\n",
    "\n",
    "# # Imprimir as métricas de avaliação\n",
    "# print(f\"Acurácia: {accuracy:.3f}\")\n",
    "# print(f\"Precisão: {precision:.3f}\")\n",
    "# print(f\"Recall: {recall:.3f}\")\n",
    "# print(f\"F1-score: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2363/1089085234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Percorrer as pastas de cada experimento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mexperiment_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\".csv\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiment_folder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_results'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from raug.metrics import Metrics, accuracy\n",
    "from raug.utils.loader import get_labels_frequency\n",
    "import pandas as pd\n",
    "\n",
    "# Especificar o caminho para a pasta \"best_results\"\n",
    "folder_path = \"best_results\"\n",
    "_csv_path_test = \"/home/giuliana/Desktop/datasets/ISIC2017/test/ISIC-2017_Test_v2_Part3_GroundTruth.csv\"\n",
    "true_labels = pd.read_csv(_csv_path_test).sort_values(by=['image_id'])['category']\n",
    "\n",
    "# Inicializar as variáveis para armazenar as previsões dos modelos\n",
    "ensemble_preds_best = None\n",
    "ensemble_preds_last = None\n",
    "num_models = 0\n",
    "ser_lab_freq = get_labels_frequency(_csv_path_test, \"category\", \"image_id\")\n",
    "_labels_name = ser_lab_freq.index.values #np.reshape(ser_lab_freq.index.values, (ser_lab_freq.index.values.shape[0], 1))\n",
    "\n",
    "df_best = pd.DataFrame()\n",
    "df_last = pd.DataFrame()\n",
    "\n",
    "# Percorrer as pastas de cada experimento\n",
    "for experiment_folder in os.listdir(folder_path):\n",
    "\n",
    "    if \".csv\" in experiment_folder:\n",
    "        continue\n",
    "\n",
    "    print(\"Experimento: \", experiment_folder)\n",
    "\n",
    "    _metric_options_best = {\n",
    "        # 'save_all_path': os.path.join(folder_path, experiment_folder, 'ensemble_3_best_average/best'),\n",
    "        'pred_name_scores': 'predictions.csv',\n",
    "    }\n",
    "    _metric_options_last = {\n",
    "        # 'save_all_path': os.path.join(folder_path, experiment_folder, 'ensemble_3_last_average/last'),\n",
    "        'pred_name_scores': 'predictions.csv',\n",
    "    }\n",
    "    options = {\n",
    "        'save_all_path': os.path.join(folder_path, experiment_folder, 'ensemble_3_best_average/best'),\n",
    "        'pred_name_scores': 'predictions.csv',\n",
    "    }\n",
    "\n",
    "\n",
    "    experiment_path = os.path.join(folder_path, experiment_folder)\n",
    "    metrics_best = Metrics (\"all\" , _labels_name, _metric_options_best)\n",
    "    metrics_last = Metrics (\"all\", _labels_name, _metric_options_last)\n",
    "    best_metric_ensemble = Metrics (\"all\", _labels_name, options)\n",
    "\n",
    "    ensemble_preds_best = None\n",
    "    ensemble_preds_last = None\n",
    "\n",
    "    experiment_folders = os.listdir(experiment_path)\n",
    "    \n",
    "    # Cria path para o experimento atual\n",
    "    if 'ensemble_average' in experiment_folders:\n",
    "        experiment_folders.remove('ensemble_average')\n",
    "    if 'ensemble_3_best_average' in experiment_folders:\n",
    "        experiment_folders.remove('ensemble_3_best_average')\n",
    "    else:\n",
    "        os.mkdir(os.path.join(folder_path, experiment_folder, 'ensemble_3_best_average'))\n",
    "        os.mkdir(os.path.join(_metric_options_best['save_all_path']))\n",
    "        os.mkdir(os.path.join(_metric_options_last['save_all_path']))\n",
    "\n",
    "\n",
    "    try:\n",
    "        best_balanced_accuracy = 0\n",
    "        best_metric_ensemble = None\n",
    "\n",
    "        # Percorrer as pastas de cada modelo\n",
    "        for i1 in range(len(experiment_folders)-1):\n",
    "            for i2 in range(i1+1, len(experiment_folders)-1):\n",
    "                for i3 in range(i2+1, len(experiment_folders)-1):\n",
    "                    model_folders = [experiment_folders[i1], experiment_folders[i2], experiment_folders[i3]]\n",
    "\n",
    "                    for model_folder in model_folders:\n",
    "\n",
    "                        model_path = os.path.join(experiment_path, model_folder)\n",
    "\n",
    "                        # Carregar as previsões do modelo atual\n",
    "                        model_preds_path_best = os.path.join(model_path, \"test_pred_best/predictions.csv\")\n",
    "                        # model_preds_path_last = os.path.join(model_path, \"test_pred_last/predictions.csv\")\n",
    "\n",
    "                        model_preds_best = pd.read_csv(model_preds_path_best).sort_values(by=['image'])\n",
    "                        # model_preds_last = pd.read_csv(model_preds_path_last).sort_values(by=['image'])\n",
    "\n",
    "                        images_id = model_preds_best[['image']]\n",
    "                        model_preds_best = model_preds_best[['0','1','2']]\n",
    "                        # model_preds_last = model_preds_last[['0','1','2']]\n",
    "\n",
    "                        # Adicionar as previsões do modelo atual ao ensemble\n",
    "                        if ensemble_preds_best is None:\n",
    "                            ensemble_preds_best = model_preds_best\n",
    "                        else:\n",
    "                            ensemble_preds_best += model_preds_best\n",
    "\n",
    "                        # if ensemble_preds_last is None:\n",
    "                        #     ensemble_preds_last = model_preds_last\n",
    "                        # else:\n",
    "                        #     ensemble_preds_last += model_preds_last\n",
    "\n",
    "                        num_models += 1\n",
    "\n",
    "                    # Calcular a média das previsões dos modelos\n",
    "                    ensemble_preds_best /= num_models\n",
    "                    ensemble_preds_last /= num_models\n",
    "\n",
    "                    metrics_best.update_scores(true_labels, ensemble_preds_best.to_numpy(), images_id.to_numpy())\n",
    "                    # metrics_last.update_scores(true_labels, ensemble_preds_last.to_numpy(), images_id.to_numpy())\n",
    "\n",
    "                    metrics_best.compute_metrics()\n",
    "                    # metrics_last.compute_metrics()\n",
    "\n",
    "                    # metrics_best.save_scores()\n",
    "                    # metrics_last.save_scores()\n",
    "\n",
    "                    if metrics_best.metrics_values['balanced_accuracy'] > best_balanced_accuracy:\n",
    "                        best_balanced_accuracy = metrics_best.metrics_values['balanced_accuracy']\n",
    "                        best_metric_ensemble = metrics_best\n",
    "                        best_experiment = experiment_folder\n",
    "\n",
    "                        ensemble_data = {\n",
    "                            'folder_name': experiment_folder,\n",
    "                            'ensemble_3_best': model_folders[0].split('fold')[0] + \"_\" + model_folders[1].split('fold')[0] + \"_\" + model_folders[2].split('fold')[0]\n",
    "                        }\n",
    "                        dict_best = {**ensemble_data, **metrics_best.metrics_values}\n",
    "                        # dict_last = {**ensemble_data, **metrics_last.metrics_values}\n",
    "\n",
    "                        del dict_best['auc_and_roc_curve']\n",
    "                        del dict_best['precision_recall_report']\n",
    "\n",
    "                        best_metric_ensemble.options = options\n",
    "                        best_metric_ensemble.compute_metrics()\n",
    "\n",
    "                        # del dict_last['auc_and_roc_curve']\n",
    "                        # del dict_last['precision_recall_report']\n",
    "\n",
    "                    # df_best = df_best.append(pd.DataFrame(dict_best, columns=dict_best.keys(), index=[0]), ignore_index=True)\n",
    "                    # df_last = df_last.append(pd.DataFrame(dict_last, columns=dict_last.keys(), index=[0]), ignore_index=True)\n",
    "\n",
    "        df_best = df_best.append(pd.DataFrame(dict_best, columns=dict_best.keys(), index=[0]), ignore_index=True)\n",
    "        # df_last = df_last.append(pd.DataFrame(dict_last, columns=dict_last.keys(), index=[0]), ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erro no experimento \", experiment_folder)\n",
    "        print(e.with_traceback())\n",
    "\n",
    "\n",
    "\n",
    "df_best.to_csv(os.path.join(folder_path, 'ensemble_3_best_average_best.csv'), index=False)\n",
    "df_last.to_csv(os.path.join(folder_path, 'ensemble_3_best_average_last.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir('best_results')\n",
    "\n",
    "folders.remove('ensemble_average_best.csv')\n",
    "folders.remove('ensemble_average_last.csv')\n",
    "\n",
    "# Percorrer as pastas de cada experimento\n",
    "for i1 in range(len(folders)):\n",
    "    for i2 in range(i1+1,len(folders)):\n",
    "        for i3 in range(i2+1,len(folders)):\n",
    "            print(fi1, i2, i3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"best_results/ensemble_average_best.csv\"\n",
    "\n",
    "df = pd.read_csv(folder_path)\n",
    "\n",
    "df.sort_values(by=['balanced_accuracy'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
